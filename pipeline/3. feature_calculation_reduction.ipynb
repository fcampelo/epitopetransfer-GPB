{"cells":[{"cell_type":"markdown","source":["# Loading datasets and model"],"metadata":{"id":"bfFjL6sj89Bu"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"tX1Qk3f9WIrU"},"outputs":[],"source":["# Download the ESM-2 model before running\n","tokenizer_model = \"./model_esm2/esm2_t33_650M_UR50D\"\n","base_model = \"./model_esm2/esm2_t33_650M_UR50D\"\n","\n","string_organism = \"B. pertussis\"\n","organism = \"bpertussis\"\n","\n","path_results = \"./results_esm2/\"\n","path_models = \"./models_esm2/\"\n","\n","method = \"epitopetransfer\"\n","method_name = \"EpitopeTransfer\"\n","\n","# generating fasta path for all folds\n","train_fasta_folds = [\"fold1\", \"fold2\", \"fold4\"]\n","test_fasta_fold = \"fold5\"\n","train_fasta_files = [f\"./fasta/folds/{organism}/{fold}.fasta\" for fold in train_fasta_folds]\n","test_fasta_file = f\"./fasta/folds/{organism}/{test_fasta_fold}.fasta\"\n","\n","#folds\n","fold_base_path = f\"./input/organismos/{organism}/folds/\"\n","train_fold_paths = [f\"{fold_base_path}fold{i}.csv\" for i in [1, 2, 4]]\n","\n","test_fold = \"fold5\"\n","test_path = f\"{fold_base_path}{test_fold}.csv\"\n","\n","base_path_predictions = f\"./predictions_esm2/{method}/{organism}\"\n","\n","path_predictions = f\"{base_path_predictions}/{test_fold}.csv\"\n","fold_numbers = 4 # number of train folds + test_fold\n","max_seq_length = 1024\n","overlap = 256"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IofTKfm0pIkl"},"outputs":[],"source":["!pip install transformers\n","!pip install fair-esm  # latest release, OR:\n","!pip install git+https://github.com/facebookresearch/esm.git\n","!pip install biopython\n","!pip install optuna-integration"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SHp-T6ps3pwt"},"outputs":[],"source":["from Bio import SeqIO\n","from transformers import AutoModel, AutoTokenizer\n","import torch\n","import numpy as np\n","\n","def predict_fasta_sequences(fasta_file):\n","\n","\n","    model = AutoModel.from_pretrained(base_model)\n","    tokenizer = AutoTokenizer.from_pretrained(tokenizer_model, do_lower_case=False)\n","\n","    fasta_sequences = SeqIO.parse(open(fasta_file), 'fasta')\n","    sequence_dict = {}\n","\n","    model.eval()  # Disable dropout\n","\n","    for record in fasta_sequences:\n","        sequence = str(record.seq)\n","        sequence_length = len(sequence)\n","        embeddings = None\n","\n","        if sequence_length > max_seq_length:\n","            # Divede the sequendo in chunks with overlap\n","            for i in range(0, sequence_length, max_seq_length - overlap):\n","                chunk = sequence[i:i + max_seq_length]\n","                if len(chunk) < max_seq_length:\n","                    chunk = chunk + ' ' * (max_seq_length - len(chunk))  # Padding if less than max_seq_length\n","                features = tokenizer.batch_encode_plus(\n","                    [chunk],\n","                    add_special_tokens=True,\n","                    padding='max_length',\n","                    max_length=max_seq_length,\n","                    truncation=True,\n","                    return_tensors='pt',\n","                    return_attention_mask=True\n","                )\n","\n","                with torch.no_grad():\n","                    outputs = model(features['input_ids'], features['attention_mask'])\n","\n","                last_hidden_state = outputs[0]\n","                array_last_hidden_state = last_hidden_state.detach().numpy()\n","\n","                if embeddings is None:\n","                    embeddings = array_last_hidden_state[0]\n","                else:\n","                    embeddings = np.concatenate((embeddings, array_last_hidden_state[0][overlap:]))\n","        else:\n","            features = tokenizer.batch_encode_plus(\n","                [sequence],\n","                add_special_tokens=True,\n","                padding='max_length',\n","                max_length=max_seq_length,\n","                truncation=True,\n","                return_tensors='pt',\n","                return_attention_mask=True\n","            )\n","\n","            with torch.no_grad():\n","                outputs = model(features['input_ids'], features['attention_mask'])\n","\n","            last_hidden_state = outputs[0]\n","            embeddings = last_hidden_state.detach().numpy()[0]\n","\n","        sequence_dict[record.id] = {\n","            'completed_sequence': sequence,\n","            'embeddings': embeddings\n","        }\n","\n","    return sequence_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ai-7UNg34GQ5"},"outputs":[],"source":["import pandas as pd\n","\n","# Select labeled regions after forward pass through the neural network\n","def select_labeled_regions(dataset, sequence_dict):\n","    list_amino_features = []\n","    list_amino_label = []\n","\n","    for idx, row in dataset.iterrows():\n","        protein_id = row[\"Info_protein_id\"]\n","\n","        if protein_id not in sequence_dict:\n","            print(f\"{protein_id} is not in sequence_dict\")\n","            continue\n","\n","        data = sequence_dict[protein_id]\n","        sequence_slice = data['embeddings']\n","        start = row[\"Info_start_pos\"]\n","        end = row[\"Info_end_pos\"]\n","        label = row[\"Class\"]\n","        sequence = data['completed_sequence']\n","\n","        for index_amino in range(start - 1, end):\n","            list_amino_features.append(sequence_slice[index_amino])\n","            list_amino_label.append(label)\n","\n","    return  list_amino_features, list_amino_label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xvz0FfYtBsQI"},"outputs":[],"source":["# Function to combine the results of multiple calls to predict_fasta_sequences\n","def predict_fasta_sequences_multiple(files):\n","    combined_sequence_dict = {}\n","    for fasta_file in files:\n","\n","        print(fasta_file)\n","        sequence_dict = predict_fasta_sequences(fasta_file)\n","        combined_sequence_dict.update(sequence_dict)\n","    return combined_sequence_dict\n","\n","train_sequence_dict = predict_fasta_sequences_multiple(train_fasta_files)\n","test_sequence_dict = predict_fasta_sequences(test_fasta_file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iU7DOIk3fIgM"},"outputs":[],"source":["import pandas as pd\n","\n","# Process each training fodl\n","for i, fold_path in enumerate(train_fold_paths, start=1):\n","\n","    df_fold = pd.read_csv(fold_path)\n","\n","\n","    processed_data = select_labeled_regions(df_fold, train_sequence_dict)\n","\n","    # Convertendo as listas em DataFrames\n","    features_df = pd.DataFrame(processed_data[0])\n","    labels_df = pd.DataFrame(processed_data[1])\n","\n","    # Salvando features e labels de cada fold em arquivos CSV\n","    features_csv_path = f\"{fold_base_path}{method}/processed_fold{i}_features.csv\"\n","    labels_csv_path = f\"{fold_base_path}{method}/processed_fold{i}_labels.csv\"\n","\n","    os.makedirs(os.path.dirname(features_csv_path), exist_ok=True)\n","    os.makedirs(os.path.dirname(labels_csv_path), exist_ok=True)\n","\n","    features_df.to_csv(features_csv_path, index=False)\n","    labels_df.to_csv(labels_csv_path, index=False)\n","\n","    print(f\"Processed fold{i} features saved to {features_csv_path}\")\n","    print(f\"Processed fold{i} labels saved to {labels_csv_path}\")\n","\n","\n","df_test = pd.read_csv(test_path)\n","\n","dados_processados = select_labeled_regions(df_test, test_sequence_dict)\n","\n","features_csv_path = f\"{fold_base_path}{method}/processed_test_features.csv\"\n","labels_csv_path = f\"{fold_base_path}{method}/processed_test_labels.csv\"\n","pd.DataFrame(dados_processados[0]).to_csv(features_csv_path, index=False)\n","pd.DataFrame({\"Label\": dados_processados[1]}).to_csv(labels_csv_path, index=False)\n","\n","print(f\"Processed test features saved to {features_csv_path}\")\n","print(f\"Processed test labels saved to {labels_csv_path}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hE8bfQ_Cg25V"},"outputs":[],"source":["import pandas as pd\n","\n","loaded_data = {}\n","\n","# Loading the traing datasets\n","for i in range(1, fold_numbers):\n","    features_csv_path = f\"{fold_base_path}{method}/processed_fold{i}_features.csv\"\n","    labels_csv_path = f\"{fold_base_path}{method}/processed_fold{i}_labels.csv\"\n","\n","    loaded_data[f\"fold{i}\"] = {\n","        \"features\": pd.read_csv(features_csv_path),\n","        \"labels\": pd.read_csv(labels_csv_path)\n","    }\n","\n","# Loading the test datasets\n","test_features_csv_path = f\"{fold_base_path}{method}/processed_test_features.csv\"\n","test_labels_csv_path = f\"{fold_base_path}{method}/processed_test_labels.csv\"\n","\n","loaded_data[\"test\"] = {\n","    \"features\": pd.read_csv(test_features_csv_path),\n","    \"labels\": pd.read_csv(test_labels_csv_path)\n","}\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"--z3ge5qjg5h"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import optuna\n","from optuna.distributions import IntDistribution, CategoricalDistribution\n","from optuna.integration.sklearn import OptunaSearchCV\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import matthews_corrcoef, make_scorer, roc_auc_score, f1_score\n","import pickle\n","import os\n","\n","\n","def perform_validation_holdout(loaded_data):\n","    X = loaded_data[\"fold1\"][\"features\"]\n","    y = loaded_data[\"fold1\"][\"labels\"].iloc[:, 0]\n","\n","    final_model = RandomForestClassifier()  # default hyperparameters\n","    final_model.fit(X, y)\n","\n","    best_threshold = 0.5\n","\n","    # Save final model\n","    os.makedirs(path_models, exist_ok=True)\n","    model_path = os.path.join(path_models, f\"{method}_{organism}.pkl\")\n","    with open(model_path, 'wb') as f:\n","        pickle.dump(final_model, f)\n","    print(f\"Final model saved to {model_path}\")\n","\n","    return final_model, best_threshold, final_model.get_params()\n","\n","def custom_mcc(estimator, X, y):\n","    preds = estimator.predict(X)\n","    return matthews_corrcoef(y, preds)\n","\n","\n","def perform_cross_validation(loaded_data):\n","\n","    all_features = pd.concat([loaded_data[f\"fold{i}\"][\"features\"] for i in range(1, len(loaded_data))], ignore_index=True)\n","    all_labels = pd.concat([loaded_data[f\"fold{i}\"][\"labels\"] for i in range(1, len(loaded_data))], ignore_index=True).iloc[:, 0]\n","\n","    fold_sizes = [len(loaded_data[f\"fold{i}\"][\"features\"]) for i in range(1, len(loaded_data))]\n","\n","    # Generates sequential indices for concatenated folds\n","    fold_indices = []\n","    current = 0\n","    for size in fold_sizes:\n","        fold_indices.append(list(range(current, current + size)))\n","        current += size\n","\n","    # Defines training and validation indices based on sequential indices\n","    train_indices_list = []\n","    val_indices_list = []\n","\n","    for i in range(len(fold_indices)):\n","        val_indices = fold_indices[i]\n","        train_indices = [idx for j in range(len(fold_indices)) if j != i for idx in fold_indices[j]]\n","        val_indices_list.append(val_indices)\n","        train_indices_list.append(train_indices)\n","\n","\n","    # Define a custom class for StratifiedKFold\n","    class CustomStratifiedKFold:\n","        def __init__(self, train_indices_list, val_indices_list):\n","            self.train_indices_list = train_indices_list\n","            self.val_indices_list = val_indices_list\n","            self.n_splits = len(train_indices_list)\n","\n","        def split(self, X, y=None, groups=None):\n","            for i in range(self.n_splits):\n","                yield self.train_indices_list[i], self.val_indices_list[i]\n","\n","        def get_n_splits(self, X=None, y=None, groups=None):\n","            return self.n_splits\n","\n","    custom_skf = CustomStratifiedKFold(train_indices_list, val_indices_list)\n","\n","    # Hyperparameters to be optimized by OptunaSearchCV\n","    param_distributions = {\n","        'n_estimators': IntDistribution(low=100, high=500),\n","        'max_depth': CategoricalDistribution(choices=[None, 10, 20, 30]),\n","        'min_samples_split': IntDistribution(low=2, high=10),\n","        'min_samples_leaf': IntDistribution(low=1, high=10),\n","        'max_features': CategoricalDistribution(choices=['sqrt', 'log2', None]),\n","        'bootstrap': CategoricalDistribution(choices=[True, False]),\n","        'criterion': CategoricalDistribution(choices=['gini', 'entropy'])\n","    }\n","\n","    model = RandomForestClassifier(random_state=42, n_jobs=-1)\n","    optuna_search = OptunaSearchCV(\n","        estimator=model,\n","        param_distributions=param_distributions,\n","        n_trials=100,\n","        random_state=42,\n","        cv=custom_skf,\n","        n_jobs=-1,\n","        scoring=custom_mcc\n","    )\n","\n","    # Perform search for the best hyperparameters using custom StratifiedKFold\n","    optuna_search.fit(all_features, all_labels)\n","\n","    # Get the best hyperparameters\n","    best_hyperparams = optuna_search.best_params_\n","    final_model = RandomForestClassifier(**best_hyperparams, random_state=42, n_jobs=-1)\n","\n","    final_model.fit(all_features, all_labels)\n","\n","    # Get the prediction probabilities for all data\n","    all_probs = final_model.predict_proba(all_features)[:, 1]\n","\n","    # Determine the best threshold by maximizing the MCC\n","    thresholds = np.arange(0, 1, 0.001)\n","\n","    mcc_scores = [matthews_corrcoef(all_labels, all_probs >= t) for t in thresholds]\n","    mcc_best_threshold_index = np.argmax(mcc_scores)\n","    mcc_best_threshold = thresholds[mcc_best_threshold_index]\n","\n","    # Save the best model\n","    model_path = os.path.join(path_models, f\"{method}_{organism}.pkl\")\n","    with open(model_path, 'wb') as f:\n","        pickle.dump(final_model, f)\n","    print(f\"Best model saved to {model_path}\")\n","\n","    return final_model, mcc_best_threshold, best_hyperparams\n","\n","def evaluate_model_on_test(test_features, test_labels, model, mcc_threshold):\n","\n","    test_probs = model.predict_proba(test_features)[:, 1]\n","    predictions_mcc = test_probs >= mcc_threshold\n","    auc = roc_auc_score(test_labels, test_probs)\n","    f1_at_mcc_threshold = f1_score(test_labels, predictions_mcc)\n","    mcc_at_mcc_threshold = matthews_corrcoef(test_labels, predictions_mcc)\n","\n","    print(f\"AUC: {auc}\")\n","    print(f\"At MCC Threshold -> F1 Score: {f1_at_mcc_threshold}, MCC: {mcc_at_mcc_threshold}\")\n","\n","    results = pd.DataFrame([{\n","        \"Method\": method,\n","        \"Organism\": organism,\n","        \"Test probabilities\": str(test_probs.tolist()),\n","        \"MCC threshold\": mcc_threshold\n","    }])\n","\n","    results_file_path = os.path.join(path_results, f\"{method}_{organism}_test_prob_thresholds.csv\")\n","\n","    if os.path.exists(results_file_path):\n","        existing_results = pd.read_csv(results_file_path)\n","        results = pd.concat([existing_results, results], ignore_index=True)\n","\n","    # Saving the results to a csv\n","    results.to_csv(results_file_path, index=False)\n","\n","    return {\n","        'auc': auc,\n","        'mcc_threshold': {'f1': f1_at_mcc_threshold, 'mcc': mcc_at_mcc_threshold}\n","    }\n","\n","\n","def main(loaded_data, organism, method, path_results):\n","    if len(loaded_data) > 2:\n","        best_model, mcc_best_threshold, best_hyperparams = perform_cross_validation(loaded_data)\n","\n","        # Loading test datasets\n","        test_features = loaded_data[\"test\"][\"features\"]\n","        test_labels = loaded_data[\"test\"][\"labels\"].iloc[:, 0]\n","\n","        # Evaluating the model on the test set\n","        results_dict = evaluate_model_on_test(test_features, test_labels, best_model, mcc_best_threshold)\n","\n","        results = pd.DataFrame({\n","            \"Dataset\":        [organism] * 3,\n","            \"Method\":         [method]   * 3,\n","            \"Metric\":         [\"AUC\",\n","                              \"F1 (MCC Threshold)\",\n","                              \"MCC (MCC Threshold)\"],\n","            \"Value\":          [results_dict['auc'],\n","                              results_dict['mcc_threshold']['f1'],\n","                              results_dict['mcc_threshold']['mcc']],\n","            \"Best threshold\": [\"\",\n","                              mcc_best_threshold,\n","                              mcc_best_threshold]\n","        })\n","\n","\n","        results[\"Value\"] = results[\"Value\"].round(3)\n","\n","        # Including the best hyperparameters as a single string\n","        best_hyperparams_str = ', '.join([f\"{key}: {value}\" for key, value in best_hyperparams.items()])\n","        results[\"Best hyperparameters\"] = [\"\"] * len(results)\n","        results.loc[0, \"Best hyperparameters\"] = best_hyperparams_str\n","\n","        results_file_path = os.path.join(path_results, \"test_metrics_summary.csv\")\n","\n","        # Checking if the file already exists and loading existing content if necessary\n","        if os.path.exists(results_file_path):\n","            existing_results = pd.read_csv(results_file_path)\n","            results = pd.concat([existing_results, results], ignore_index=True)\n","\n","        results.to_csv(results_file_path, index=False)\n","\n","        print(f\"Results saved to {results_file_path}\")\n","    else:\n","        best_model, mcc_best_threshold, best_hyperparams = perform_validation_holdout(loaded_data)\n","\n","        test_features = loaded_data[\"test\"][\"features\"]\n","        test_labels = loaded_data[\"test\"][\"labels\"].iloc[:, 0]\n","\n","        results_dict = evaluate_model_on_test(test_features, test_labels, best_model, mcc_best_threshold)\n","\n","        results = pd.DataFrame({\n","            \"Dataset\":        [organism] * 3,\n","            \"Method\":         [method]   * 3,\n","            \"Metric\":         [\"AUC\",\n","                              \"F1 (MCC Threshold)\",\n","                              \"MCC (MCC Threshold)\"],\n","            \"Value\":          [results_dict['auc'],\n","                              results_dict['mcc_threshold']['f1'],\n","                              results_dict['mcc_threshold']['mcc']],\n","            \"Best threshold\": [\"\",\n","                              mcc_best_threshold,\n","                              mcc_best_threshold]\n","        })\n","\n","\n","        results[\"Value\"] = results[\"Value\"].round(3)\n","\n","        best_hyperparams_str = ', '.join([f\"{key}: {value}\" for key, value in best_hyperparams.items()])\n","        results[\"Best hyperparameters\"] = [\"\"] * len(results)\n","        results.loc[0, \"Best hyperparameters\"] = best_hyperparams_str\n","\n","        results_file_path = os.path.join(path_results, \"test_metrics_summary.csv\")\n","\n","        if os.path.exists(results_file_path):\n","            existing_results = pd.read_csv(results_file_path)\n","            results = pd.concat([existing_results, results], ignore_index=True)\n","\n","        results.to_csv(results_file_path, index=False)\n","\n","        print(f\"Results saved to {results_file_path}\")\n","\n","main(loaded_data, organism, method, path_results)\n"]}],"metadata":{"accelerator":"TPU","colab":{"gpuType":"V28","machine_shape":"hm","provenance":[{"file_id":"1AGCrXUJcWqp6JPe2Q1zkC-g66LZfGxlZ","timestamp":1682464104036}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}