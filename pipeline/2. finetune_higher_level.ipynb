{"cells":[{"cell_type":"markdown","source":["# Loading datasets and model"],"metadata":{"id":"jG1ypzDc8xs-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"XcSy2IcbWP59"},"outputs":[],"source":["!pip install transformers\n","!pip install --upgrade accelerate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K-58eIcAl8x-"},"outputs":[],"source":["# Download the ESM-2 model before running\n","tokenizer_model = \"./model_esm2/esm2_t33_650M_UR50D\"\n","base_model = \"./model_esm2/esm2_t33_650M_UR50D\"\n","\n","# higher-level\n","input_training_data = \"./input/organismos/bpertussis/train_pseudomonadota_minus_pertussis.csv\"\n","input_test_data = \"./input/organismos/bpertussis/test_pseudomonadota_minus_pertussis.csv\"\n","generated_model = \"./model_esm2/bpertussis_pseudomonadota_minus_bpertussis\"\n"]},{"cell_type":"markdown","metadata":{"id":"hla_98p0gbT6"},"source":["# Supporting functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xXfagt2R4ysX"},"outputs":[],"source":["import torch\n","\n","# Create torch dataset\n","class Dataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels=None):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        if self.labels:\n","            item[\"labels\"] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.encodings[\"input_ids\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BjynGg7iFO5Y"},"outputs":[],"source":["\n","# Passing throuth the label region considering the window size on the whole protein\n","def segmentBycontext(sequences, start, end, classes,  window):\n","\n","    center = []\n","    left = []\n","    right = []\n","    labels = []\n","    count=0;\n","    window_size = window\n","\n","    for sequence, start, end, label in zip(sequences, start, end, classes):\n","        length_sequence = len(sequence)\n","        position=start-1;\n","\n","        while position < end:\n","\n","            center_amino = sequence[position]\n","\n","            if position > window_size:\n","                left_aminos = sequence[position - window_size:position]\n","\n","            else:\n","                left_aminos = sequence[0:position]\n","\n","            if window_size < (length_sequence - position) :\n","                right_aminos = sequence[position + 1:position + 1 + window_size]\n","\n","            else:\n","                right_aminos = sequence[position + 1:length_sequence]\n","\n","\n","            center.append(center_amino)\n","            left.append(left_aminos)\n","            right.append(right_aminos)\n","            labels.append(label)\n","\n","            position = position + 1;\n","            count = count + 1;\n","\n","\n","    return center, left, right, labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L-PdPWMVFSs0"},"outputs":[],"source":["def process_data_for_bert(data, tokenizer):\n","\n","    sentence_center = list(data[\"center\"])\n","    sentence_left = list(data[\"left\"])\n","    sentence_right = list(data[\"right\"])\n","\n","    left_center_right = []\n","\n","    for (left, center, right) in zip(sentence_left, sentence_center, sentence_right):\n","        left_center_right.append(left + center + right)\n","\n","    y = list(data[\"label\"])\n","\n","    X_tokenized = tokenizer(left_center_right, padding=True, truncation=True, max_length=512)\n","    dataset = Dataset(X_tokenized, y)\n","\n","\n","    return dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZZyAnSrAFm4a"},"outputs":[],"source":["def data_processing_pipeline(data, window_size, tokenizer):\n","\n","    start = list(data[\"Info_start_pos\"])\n","    end = list(data[\"Info_end_pos\"])\n","\n","    sequence = data['Info_sequence'].tolist()\n","    labels = data['Class'].tolist()\n","\n","    center, left, right, labels =  segmentBycontext(sequence, start, end, labels, window_size)\n","\n","    sentences = pd.DataFrame({'center':center, 'left':left, 'right':right, 'label':labels})\n","\n","    dataset = process_data_for_bert(sentences, tokenizer)\n","\n","\n","    return dataset"]},{"cell_type":"markdown","source":["# Fine-tune pretrained model"],"metadata":{"id":"m93Whczmvh-I"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":154779,"status":"ok","timestamp":1747580300985,"user":{"displayName":"Lindeberg Leite","userId":"13691959383703340166"},"user_tz":180},"id":"DVekXFaKy38u","outputId":"e88a7268-5592-4570-d030-95d6c5b53582"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at ./model_esm2/esm2_t33_650M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","<ipython-input-11-ff7ff8948a73>:58: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]}],"source":["import pandas as pd\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from transformers import TrainingArguments, Trainer\n","from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n","from transformers import AutoModelForSequenceClassification\n","import numpy as np\n","from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, matthews_corrcoef\n","\n","batch_size=8\n","\n","tokenizer = AutoTokenizer.from_pretrained(tokenizer_model, do_lower_case=False)\n","model = AutoModelForSequenceClassification.from_pretrained(base_model, num_labels=2)\n","\n","model.cuda()\n","\n","train_dataset = pd.read_csv(input_training_data)\n","val_dataset = pd.read_csv(input_test_data)\n","\n","train_dataset = data_processing_pipeline(train_dataset, 512, tokenizer)\n","val_dataset = data_processing_pipeline(val_dataset, 512, tokenizer)\n","\n","# Define Trainer parameters\n","def compute_metrics(p):\n","    pred, labels = p\n","    pred = np.argmax(pred, axis=1)\n","\n","    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n","    recall = recall_score(y_true=labels, y_pred=pred)\n","    precision = precision_score(y_true=labels, y_pred=pred)\n","    f1 = f1_score(y_true=labels, y_pred=pred)\n","    mcc = matthews_corrcoef(y_true=labels, y_pred=pred)\n","\n","    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1, \"mcc\": mcc}\n","\n","\n","args = TrainingArguments(\n","    generated_model,\n","    report_to=\"none\",\n","    eval_strategy = \"epoch\",\n","    save_strategy = \"epoch\",\n","    learning_rate=1e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    num_train_epochs=3,\n","    weight_decay=0.01,\n","    load_best_model_at_end=True,\n","    metric_for_best_model='f1',\n","    optim=\"adamw_torch\"\n",")\n","\n","def model_init():\n","    return model\n","\n","trainer = Trainer(\n","    model_init=model_init,\n","    args=args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")\n","\n","# New code - wrap collator in a dictionary\n","data_collator = trainer.data_collator\n","trainer.data_collator = lambda data: dict(data_collator(data))\n","# End new code\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"cxPDT8TI1hMR"},"outputs":[],"source":["trainer.train()\n","\n","# save the fine-tuned model trained on the higher dataset\n","trainer.save_model(generated_model)"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}